{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cnD1ecF0Mw0u"
   },
   "source": [
    "# Feature Selection and LASSO (Interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6oYlO_fMw0z"
   },
   "source": [
    "In this notebook, you will use LASSO to select features, building on a pre-implemented solver for LASSO (in sklearn, obviously). You will:\n",
    "* Run LASSO with different L1 penalties.\n",
    "* Choose best L1 penalty using a validation set.\n",
    "* Choose best L1 penalty using a validation set, with additional constraint on the size of subset.\n",
    "\n",
    "In the next exercise, you will implement your own LASSO solver, using coordinate descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wRSBur8HMw02"
   },
   "source": [
    "## The usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjaAEidBMw05"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBmiSN8sMw1A"
   },
   "source": [
    "## Load in house sales data\n",
    "\n",
    "Dataset is from house sales in King County, the region where the city of Seattle, WA is located. *I am so surprised.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7cDLiqAIMw1D",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_data = pandas.read_csv(\"kc_house_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nyLeYGvlMw1J"
   },
   "source": [
    "## Create new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSMOdbHVMw1L"
   },
   "source": [
    "As in Lab 2 (*lab-2.ipynb*), we consider features that are some transformations of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mYzffDTvMw1M"
   },
   "outputs": [],
   "source": [
    "from math import log, sqrt\n",
    "full_data['sqft_living_sqrt'] = full_data['sqft_living'].map(sqrt)\n",
    "full_data['sqft_lot_sqrt'] = full_data['sqft_lot'].map(sqrt)\n",
    "full_data['bedrooms_square'] = full_data['bedrooms'] ** 2\n",
    "\n",
    "# In the dataset, 'floors' was defined with type string, \n",
    "# so we'll convert them to float, before creating a new feature.\n",
    "full_data['floors'] = full_data['floors'].astype(float) \n",
    "full_data['floors_square'] = full_data['floors'] ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z672nh0kMw1S"
   },
   "source": [
    "* Squaring bedrooms will increase the separation between not many bedrooms (e.g. 1) and lots of bedrooms (e.g. 4) since 1^2 = 1 but 4^2 = 16. Consequently this variable will mostly affect houses with many bedrooms.\n",
    "* On the other hand, taking square root of sqft_living will decrease the separation between big house and small house. The owner may not be exactly twice as happy for getting a house that is twice as big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VDo2w5chMw1U"
   },
   "source": [
    "# Learn regression weights with L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u-xMl9bUMw1V"
   },
   "source": [
    "Let us fit a model with all the features available, plus the features we just created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYAUT1PLMw1W"
   },
   "outputs": [],
   "source": [
    "all_features = ['bedrooms', 'bedrooms_square',\n",
    "            'bathrooms',\n",
    "            'sqft_living', 'sqft_living_sqrt',\n",
    "            'sqft_lot', 'sqft_lot_sqrt',\n",
    "            'floors', 'floors_square',\n",
    "            'waterfront', 'view', 'condition', 'grade',\n",
    "            'sqft_above',\n",
    "            'sqft_basement',\n",
    "            'yr_built', 'yr_renovated']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7-592ikMw1e"
   },
   "source": [
    "Applying L1 penalty requires adding an extra parameter (`alpha=l1_penalty`) to the sklearn model `Lasso`. (Other tools may have separate implementations of LASSO). Much like L2/Ridge Regression, the features should be scaled to ensure equal attention inbetween."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlTbDC0RMw1g"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "l1_penalty=5e4\n",
    "full_features = scaler.fit_transform(full_data[all_features].values)\n",
    "full_labels = full_data['price'].values\n",
    "model_full = Lasso(alpha=l1_penalty).fit(full_features, full_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rH4847CcMw1m"
   },
   "source": [
    "Find what features had non-zero weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HElqPj8LMw1n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0.              0.              0.         132571.09360631\n",
      "      0.             -0.             -0.              0.\n",
      "      0.          14623.33961421  29004.06421249      0.\n",
      "  90207.54789031      0.              0.         -10722.34912003\n",
      "      0.        ]\n",
      "[132571.09360631  14623.33961421  29004.06421249  90207.54789031\n",
      " -10722.34912003]\n"
     ]
    }
   ],
   "source": [
    "# Do you know that even numpy has built-in boolean selector?\n",
    "coefficients = model_full.coef_\n",
    "print(coefficients)\n",
    "print(coefficients[model_full.coef_.nonzero()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0gF5MD_Mw1r"
   },
   "source": [
    "Note that a majority of the weights have been set to zero. So by setting an L1 penalty that's large enough, we are performing a subset selection. \n",
    "\n",
    "***QUIZ QUESTION***:\n",
    "According to this list of weights, which of the features have been chosen? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Zv3CFZBMw1s"
   },
   "source": [
    "# Selecting an L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oc6Vm3RfMw1t"
   },
   "source": [
    "To find a good L1 penalty, we will explore multiple values using a validation set. Let us do three way split into train, validation, and test sets:\n",
    "* Split our sales data into 2 sets: training and test\n",
    "* Further split our training data into two sets: train, validation\n",
    "\n",
    "Be *very* careful that you use seed = 1 to ensure you get the same answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U41_wuIcMw1v"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_and_validation, test_set = train_test_split(full_data, test_size=0.1, random_state=1)\n",
    "train_set, validation_set = train_test_split(train_and_validation, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = scaler.transform(train_set[all_features].values)\n",
    "train_labels= train_set['price'].values\n",
    "valid_features = scaler.transform(validation_set[all_features].values)\n",
    "valid_labels = validation_set['price'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7kCoG3VwMw10"
   },
   "source": [
    "# Next, we write a loop that does the following:\n",
    "* For `l1_penalty` in 21 steps range between [1, 10^9] (use `np.logspace(0, 9, num=21)`.)\n",
    "    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `alpha=l1_penalty` in the parameter.\n",
    "    * Compute the RSS on VALIDATION data (here you will want to use `.predict()`) for that `l1_penalty`\n",
    "* Report which `l1_penalty` produced the lowest RSS on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kYf8zDJtMw11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14654475716861.0, tolerance: 127509489470.13391\n",
      "  positive)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14663181882447.375, tolerance: 127509489470.13391\n",
      "  positive)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14687653596508.062, tolerance: 127509489470.13391\n",
      "  positive)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14755977786966.062, tolerance: 127509489470.13391\n",
      "  positive)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14943361906542.062, tolerance: 127509489470.13391\n",
      "  positive)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15430211624776.625, tolerance: 127509489470.13391\n",
      "  positive)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16474937844501.25, tolerance: 127509489470.13391\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "l1_penalty_values = np.logspace(0, 9, num=21)\n",
    "min_error = None\n",
    "best_l1_penalty = None\n",
    "validation_errors = []\n",
    "for l1_penalty in l1_penalty_values:\n",
    "    model = Lasso(alpha=l1_penalty).fit(train_features, train_labels)\n",
    "    price_predicted = model.predict(valid_features)\n",
    "    RSS = ((price_predicted-valid_labels)**2).sum()\n",
    "    validation_errors.append(RSS)\n",
    "    if min_error is None or RSS < min_error:\n",
    "        min_error = RSS\n",
    "        best_l1_penalty = l1_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAneHq6LMw15"
   },
   "source": [
    "*** QUIZ QUESTION. *** What was the best value for the `l1_penalty`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oZ1BXWL7Mw16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[416604893858220.7, 416599305725099.06, 416583670464101.6, 416540511736147.9, 416426081845074.7, 416160829225389.4, 415868035478379.56, 416826257573209.1, 434567910677354.8, 462926682778278.9, 509944971743515.25, 685404850598010.8, 1269269269066234.5, 1290758247403054.5, 1290758247403054.5, 1290758247403054.5, 1290758247403054.5, 1290758247403054.5, 1290758247403054.5, 1290758247403054.5, 1290758247403054.5]\n",
      "501.18723362727246\n",
      "415868035478379.56\n"
     ]
    }
   ],
   "source": [
    "print(validation_errors)\n",
    "print(best_l1_penalty)\n",
    "print(min_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PcnSvUHyMw19"
   },
   "source": [
    "***QUIZ QUESTION***\n",
    "Also, using this value of L1 penalty, how many nonzero weights do you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Od-dOA-IMw1-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16474937844501.25, tolerance: 127509489470.13391\n",
      "  positive)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ -14631.87620799,    4773.51870901,   41675.78855477,\n",
       "        436207.06682844, -410670.42743841,   20538.99499836,\n",
       "        -38906.99479505,  -16211.26503473,   28604.33669179,\n",
       "         39232.71729436,   28771.88041433,   17793.15256962,\n",
       "        151123.85682761,   96018.62012972,   54469.89931089,\n",
       "        -93946.55668381,    5653.56858066])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_best_L1 = Lasso(alpha=best_l1_penalty).fit(train_features, train_labels)\n",
    "model_best_L1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(model_best_L1.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_zNo9ywLMw2F"
   },
   "source": [
    "# Limit the number of nonzero weights\n",
    "\n",
    "What if we absolutely wanted to limit ourselves to, say, 5 features? This may be important if we want to derive \"a rule of thumb\" --- an interpretable model that has only a few features in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jLijk-3UMw2G"
   },
   "source": [
    "In this section, you are going to implement a simple, two phase procedure to achive this goal:\n",
    "1. Explore a large range of `l1_penalty` values to find a narrow region of `l1_penalty` values where models are likely to have the desired number of non-zero weights.\n",
    "2. Further explore the narrow region you found to find a good value for `l1_penalty` that achieves the desired sparsity.  Here, we will again use a validation set to choose the best value for `l1_penalty`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOwQHIzMMw2H"
   },
   "outputs": [],
   "source": [
    "max_nonzeros = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KMv9UnluMw2N"
   },
   "source": [
    "## Exploring the larger range of values to find a narrow range with the desired sparsity\n",
    "\n",
    "Let's define a wide range of possible `l1_penalty_values`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vEaNt8VHMw2O"
   },
   "outputs": [],
   "source": [
    "l1_penalty_values = np.logspace(3, 5, num=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mxrj8371Mw2S"
   },
   "source": [
    "Now, implement a loop that search through this space of possible `l1_penalty` values:\n",
    "\n",
    "* For `l1_penalty` in `np.logspace(3, 5, num=21)`:\n",
    "    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `alpha=l1_penalty` in the parameter.\n",
    "    * Extract the weights of the model and count the number of nonzeros. Save the number of nonzeros to a list.\n",
    "        * *Hint: `model.coef_` gives you the coefficients/parameters you learned (barring intercept) in the form of numpy array. You can then use array\\[condition\\] for the list of values passing the condition. Or just use the builtin `np.count_nonzero()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_kFzVsaiMw2U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000.0\n",
      "# Non zeros : 15\n",
      "1258.9254117941675\n",
      "# Non zeros : 15\n",
      "1584.893192461114\n",
      "# Non zeros : 15\n",
      "1995.2623149688789\n",
      "# Non zeros : 15\n",
      "2511.88643150958\n",
      "# Non zeros : 15\n",
      "3162.2776601683795\n",
      "# Non zeros : 14\n",
      "3981.0717055349733\n",
      "# Non zeros : 13\n",
      "5011.872336272725\n",
      "# Non zeros : 13\n",
      "6309.57344480193\n",
      "# Non zeros : 10\n",
      "7943.282347242814\n",
      "# Non zeros : 10\n",
      "10000.0\n",
      "# Non zeros : 10\n",
      "12589.254117941662\n",
      "# Non zeros : 10\n",
      "15848.93192461114\n",
      "# Non zeros : 6\n",
      "19952.62314968879\n",
      "# Non zeros : 6\n",
      "25118.864315095823\n",
      "# Non zeros : 5\n",
      "31622.776601683792\n",
      "# Non zeros : 5\n",
      "39810.71705534969\n",
      "# Non zeros : 5\n",
      "50118.72336272725\n",
      "# Non zeros : 5\n",
      "63095.7344480193\n",
      "# Non zeros : 4\n",
      "79432.82347242821\n",
      "# Non zeros : 3\n",
      "100000.0\n",
      "# Non zeros : 2\n",
      "Best l1_penalty: 1000.0\n",
      "Best RSS: 416113886400657.500000\n"
     ]
    }
   ],
   "source": [
    "min_error = None\n",
    "best_l1_penalty = None\n",
    "validation_errors = []\n",
    "for l1_penalty in l1_penalty_values:\n",
    "    print (l1_penalty)\n",
    "    model = Lasso(alpha=l1_penalty).fit(train_features, train_labels)\n",
    "    print (\"# Non zeros : \" + str(np.count_nonzero(model.coef_)))\n",
    "    \n",
    "    price_predicted = model.predict(valid_features)\n",
    "    RSS = ((price_predicted-valid_labels)**2).sum()\n",
    "    validation_errors.append(RSS)\n",
    "    if min_error is None or RSS < min_error:\n",
    "        min_error = RSS\n",
    "        best_l1_penalty = l1_penalty\n",
    "\n",
    "print (\"Best l1_penalty: \" + str(best_l1_penalty))\n",
    "print (\"Best RSS: \" + str('{0:f}'.format(min_error)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ucgWXpj9Mw2Z"
   },
   "source": [
    "Out of this large range, we want to find the two ends of our desired narrow range of `l1_penalty`.  At one end, we will have `l1_penalty` values that have too few non-zeros, and at the other end, we will have an `l1_penalty` that has too many non-zeros.  \n",
    "\n",
    "More formally, find:\n",
    "* The smallest `l1_penalty` that has non-zeros equal `max_nonzeros` (if we pick a penalty smaller than this value, we will definitely have too many non-zero weights)\n",
    "    * Store this value in the variable `l1_penalty_min` (we will use it later)\n",
    "* The biggest `l1_penalty` that has non-zeros equal `max_nonzeros` (if we pick a penalty larger than this value, we will definitely have too few non-zero weights)\n",
    "    * Store this value in the variable `l1_penalty_max` (we will use it later)\n",
    "\n",
    "\n",
    "*Hint: there are many ways to do this, e.g.:*\n",
    "* Programmatically within the loop above\n",
    "* Creating a list with the number of non-zeros for each value of `l1_penalty` and inspecting it to find the appropriate boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UBpwiXEVMw2b"
   },
   "outputs": [],
   "source": [
    "l1_penalty_min = 19952.62314968879\n",
    "l1_penalty_max = 50118.72336272725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50119.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(50118.72336272725,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-S2jtklaMw2g"
   },
   "source": [
    "***QUIZ QUESTION.*** What values did you find for `l1_penalty_min` and `l1_penalty_max`, respectively? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-T_xUbJ5Mw2i"
   },
   "source": [
    "## Exploring the narrow range of values to find the solution with the right number of non-zeros that has lowest RSS on the validation set \n",
    "\n",
    "We will now explore the narrow region of `l1_penalty` values we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mEICQwrAMw2k"
   },
   "outputs": [],
   "source": [
    "l1_penalty_values = np.linspace(l1_penalty_min,l1_penalty_max,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IVkWedu9Mw2p"
   },
   "source": [
    "* For `l1_penalty` in `np.linspace(l1_penalty_min,l1_penalty_max,20)`:\n",
    "    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `alpha=l1_penalty`.\n",
    "    * Measure the RSS of the learned model on the VALIDATION set\n",
    "\n",
    "Find the model that the lowest RSS on the VALIDATION set and has sparsity *equal* to `max_nonzeros`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKsCKTP3Mw2q"
   },
   "outputs": [],
   "source": [
    "min_error = None\n",
    "best_l1_penalty = None\n",
    "for l1_penalty in l1_penalty_values:\n",
    "    model = Lasso(alpha=l1_penalty).fit(train_features, train_labels)\n",
    "    price_predicted = model.predict(valid_features)\n",
    "    RSS = ((price_predicted-valid_labels)**2).sum()\n",
    "    if (min_error is None or RSS < min_error) and (np.count_nonzero(model.coef_) == max_nonzeros):\n",
    "        min_error = RSS\n",
    "        best_l1_penalty = l1_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25118.864315095823\n"
     ]
    }
   ],
   "source": [
    "print(best_l1_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25119.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(best_l1_penalty,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b4vAuKXfMw2u"
   },
   "source": [
    "***QUIZ QUESTIONS***\n",
    "1. What value of `l1_penalty` in our narrow range has the lowest RSS on the VALIDATION set and has sparsity *equal* to `max_nonzeros`?\n",
    "2. What features in this model have non-zero coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VISUoAMIMw2v"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    -0.        ,      0.        ,      0.        , 143610.86073049,\n",
       "            0.        ,     -0.        ,     -0.        ,      0.        ,\n",
       "            0.        ,  26127.21193655,  30525.04632111,      0.        ,\n",
       "       121459.14456365,      0.        ,      0.        , -53061.33575139,\n",
       "            0.        ])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_max_nonzeros = Lasso(alpha=best_l1_penalty).fit(train_features, train_labels)\n",
    "model_max_nonzeros.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "lab-5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
